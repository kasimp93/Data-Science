{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import mixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances as pairdist\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading Training Data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train = df_train.iloc[0:460804,:]\n",
    "#df = df.dropna()\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_train.head()\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "f_1 = df_train['HelpfulnessDenominator'].tolist()\n",
    "f_2 = df_train['HelpfulnessNumerator'].tolist()\n",
    "f_3 = df_train['Id'].tolist()\n",
    "f_4 = df_train['Time'].tolist()\n",
    "f_1 = np.asarray(f_1)\n",
    "f_2 = np.asarray(f_2)\n",
    "f_3 = np.asarray(f_3)\n",
    "f_4 = np.asarray(f_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\kasimp93\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "df_train['sentiment_dict'] = df_train['Text'].apply(sent_analyzer.polarity_scores)\n",
    "f_6 = df_train['sentiment_dict'].apply(lambda x: x['neg'])\n",
    "f_7 = df_train['sentiment_dict'].apply(lambda x: x['pos'])\n",
    "f_8 = df_train['sentiment_dict'].apply(lambda x: x['neu'])\n",
    "f_9 = df_train['sentiment_dict'].apply(lambda x: x['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "reviews_list = df_train['Text'].tolist()\n",
    "reviews_list = [rev.translate(remove_digits) for rev in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1,1), stop_words='english')\n",
    "dtm = vectorizer.fit_transform(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lsa = TruncatedSVD(50)\n",
    "# lsa_t = lsa.fit_transform(dtm)\n",
    "# # normalize category columns and rescale to give location a larger weight\n",
    "# lsa_t = Normalizer(copy=False).fit_transform(lsa_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # stemming\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# reviews_list = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "#          for sent in sent_tokenize(rev)\n",
    "#         for word in word_tokenize(sent))\n",
    "#         for rev in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460804, 104827)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = line.translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())\n",
    "    for t in line:\n",
    "        stemmed = lemmatizer.lemmatize(t)\n",
    "        tokens.append(stemmed)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lem_1 = []\n",
    "for rev in reviews_list:\n",
    "    data_lem_1.append(preprocessing(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(1,501, 10, dtype='int16')\n",
    "lsa = TruncatedSVD(n_components=50, algorithm = 'randomized')\n",
    "dtm1 = dtm.asfptype()\n",
    "dtm_lsa = lsa.fit_transform(dtm1)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading Training Data\n",
    "df_test = pd.read_csv('train.csv')\n",
    "df_test = df_test.iloc[460804:560804,:]\n",
    "#df = df.dropna()\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test.head()\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "f_1_test = df_test['HelpfulnessDenominator'].tolist()\n",
    "f_2_test = df_test['HelpfulnessNumerator'].tolist()\n",
    "f_3_test = df_test['Id'].tolist()\n",
    "f_4_test = df_test['Time'].tolist()\n",
    "f_1_test = np.asarray(f_1_test)\n",
    "f_2_test = np.asarray(f_2_test)\n",
    "f_3_test = np.asarray(f_3_test)\n",
    "f_4_test = np.asarray(f_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "df_test['sentiment_dict'] = df_test['Text'].apply(sent_analyzer.polarity_scores)\n",
    "f_6_test = df_test['sentiment_dict'].apply(lambda x: x['neg'])\n",
    "f_7_test = df_test['sentiment_dict'].apply(lambda x: x['pos'])\n",
    "f_8_test = df_test['sentiment_dict'].apply(lambda x: x['neu'])\n",
    "f_9_test = df_test['sentiment_dict'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "remove_digits_test = str.maketrans('', '', digits)\n",
    "reviews_list_test = df_test['Summary'].tolist()\n",
    "reviews_list_test = [rev.translate(remove_digits_test) for rev in reviews_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_test = TfidfVectorizer(decode_error='ignore', ngram_range=(1,1), stop_words='english')\n",
    "dtm_test = vectorizer_test.fit_transform(reviews_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa_test = TruncatedSVD(50, algorithm = 'randomized')\n",
    "lsa_t_test = lsa_test.fit_transform(dtm_test)\n",
    "# normalize category columns and rescale to give location a larger weight\n",
    "lsa_t_test = Normalizer(copy=False).fit_transform(lsa_t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # stemming\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# reviews_list_test = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "#          for sent in sent_tokenize(rev)\n",
    "#         for word in word_tokenize(sent))\n",
    "#         for rev in reviews_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = line.translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())\n",
    "    for t in line:\n",
    "        stemmed = lemmatizer.lemmatize(t)\n",
    "        tokens.append(stemmed)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lem_1_test = []\n",
    "for rev in reviews_list_test:\n",
    "    data_lem_1_test.append(preprocessing(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(1,501, 10, dtype='int16')\n",
    "lsa_test = TruncatedSVD(n_components=50, algorithm = 'randomized')\n",
    "dtm1_test = dtm_test.asfptype()\n",
    "dtm_lsa_test = lsa_test.fit_transform(dtm1_test)\n",
    "dtm_lsa_test = Normalizer(copy=False).fit_transform(dtm_lsa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Concatenate Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_location = Normalizer(copy=False).fit_transform(np.column_stack((f_1, f_2, f_3, f_4, f_6, f_7, f_8, f_9)))\n",
    "dtm_final = np.concatenate((dtm_lsa, centered_location), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_lsa_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Concatenate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_location_test = Normalizer(copy=False).fit_transform(np.column_stack((f_1_test, f_2_test, f_3_test, f_4_test, f_6_test, f_7_test, f_8_test, f_9_test)))\n",
    "dtm_final_test = np.concatenate((dtm_lsa_test, centered_location_test), axis=1)\n",
    "X_test = dtm_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = dtm_final\n",
    "#Training Labels\n",
    "Y_train = df_train['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error is 1.2189422803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model_n = MLPClassifier(hidden_layer_sizes=(10,5), alpha=1,max_iter=20000)\n",
    "results_n = model_n.fit(X_train, Y_train)\n",
    "y_predict_final = results_n.predict(X_train)\n",
    "MSE_train = mean_squared_error(Y_train, y_predict_final)\n",
    "print(\"Mean Square Error is\", MSE_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# model_n = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes=(100, ), alpha=1, max_iter=200)\n",
    "# results_n = model_n.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "# model_n = SVR(kernel= 'linear', C=1.0)\n",
    "# results_n = model_n.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_final = results_n.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in y_predict_final:\n",
    "    res.append(i)\n",
    "    \n",
    "for i in range(len(res)):\n",
    "    if(res[i] > 5):\n",
    "        res[i] = 5\n",
    "res = [int(round(i,)) for i in res]\n",
    "test_labels = df_test['Id']        \n",
    "test_labels_1 =[]\n",
    "test_labels_1 = test_labels.values.tolist()\n",
    "\n",
    "with open('submission.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id','Score'])\n",
    "with open('submission.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "    for i in range(100000):\n",
    "        writer = csv.writer(csvfile)    \n",
    "        writer.writerow([test_labels_1[i], res[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
